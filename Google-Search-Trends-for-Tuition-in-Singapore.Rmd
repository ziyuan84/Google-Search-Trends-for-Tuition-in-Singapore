---
title: "Google Search Trends for Tuition in Singapore"
#author: "Zy"
date: "2023-04-20"
output: 
  html_document:
    keep_md: true
monofont: "Roboto Mono"
header-includes:
  - \usepackage{fontspec}
  - \newfontfamily\urlfont{Roboto Mono}
---

<style type="text/css">
  body{
  font-size: 10pt;
}
</style>

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

\urlstyle{tt}

## Abstract

<!-- <style>pre {width: 600px; border: 0; display: inline; margin: 0; padding: 0; white-space: normal}</style> -->

According to the Household Expenditure Survey (HES) conducted from 2017 to 2018 by the Singapore Department of Statistics (DOS), households in Singapore on average spend S\$112 per month on private tuition and other educational courses. Furthermore, household consumption expenditure on education in Singapore rose by 84% between 2010 and 2021. In light of the burgeoning tuition industry in Singapore, this project investigated whether there has been a similar increasing trend in the monthly frequency of Google searches in Singapore for the term "tuition" since 2010. The main goal was to identify any polynomial trends in Google searches for this term, as well as any seasonal components. Based on observations of the Google Trends data extracted, we conjectured that the normalized frequency of monthly searches for "tuition" follows a linearly decreasing trend with a seasonal component of period 12 months. 

We built a model consisting of a polynomial trend of order one as well as a seasonal component with period 12 months. It was assumed that the observational noise followed a normal distribution with mean 0 and constant but unknown variance $v$. The system covariance matrix was also assumed to be unknown and specified via discount factors. We then carried out a Bayesian analysis to obtain the filtering, smoothing and forecasting distributions under our model. The plot of the means of the smoothing distribution exhibited, in agreement with the plot of the actual data, quasi-periodic behaviour together with a decreasing trend. The plot of the means of the filtering distribution showed a similar trend, though it was generally less smooth than the corresponding plot for the smoothing distribution. The last 5 observations of the dataset were reserved for prediction; with respect to the given model, the mean square error of the forecast function was found to be approximately 42.8, compared to the approximate variance of 136 of the entire dataset. We concluded that the model consisting of a linearly decreasing trend and a seasonal component with period 12 months and  4 harmonics fitted the data fairly well.  

## Introduction

Given that Singapore household expenditure on tuition grew from S$1.1 billion in 2012/2013 to S\$1.4 billion in 2017/2018, we were interested in finding out whether
there has been a similar increase in monthly frequency of Google searches for "tuition". An inspection of a plot of the data revealed that, despite the overall increase in household expenditure on tuition since 2010, there has been a general decline in monthly searches for the term "tuition". Monthly searches for this term also display quasi-periodic behaviour, typically peaking near the start of the year and then bottoming out near the start of the fourth quarter of the year. An intermediate goal of this project was to determine how well a model consisting of a polynomial trend and a seasonal component fitted the data, and to choose the optimal order of the polynomial component and the optimal number of harmonics for the seasonal component. We proposed a model consisting of a linear trend and a seasonal component with period 12.  After experimenting with various choices of harmonics, we selected 3 additional harmonics for a total of 4 frequencies: $\omega_1 = \frac{2\pi}{12}$, $\omega_2 = \frac{2\pi}{6}$, $\omega_3 = \frac{2\pi}{4}$ and $\omega_4 = \frac{2\pi}{3}$.            


## Data Description and Exploratory Data Analysis

The following graph plots the normalized frequency of Google searches for the term "tuition" versus date.

```{r include=FALSE}
setwd("C:/Users/Owner")
res = read.csv("googletrends_tuitioninsingapore_multiTimeline.csv", header=TRUE)
res1 = data.frame(res[c(-1),])
rownames(res1) = rownames(res)[-1]
colnames(res1) = res[1,1]
l = 5
res2 = head(res1, -l)
library(lubridate)
library(dlm)
res1$YearMonth <- as.Date(ym(rownames(res1)), format="%Y-%m")
```

```{r message=FALSE, echo=FALSE, results='hide'}
plot(res1$YearMonth, res1$`tuition: (Singapore)`, type="l")
data = list(yt = as.double(res1$`tuition: (Singapore)`))
```

The graph appears to follow a quasi-periodic and decreasing trend. Based on these graph properties, we postulated a model with a linear trend and a seasonal component of period 12. To justify the seasonal component, we reasoned that demand for tuition might peak around the start of the new school year and then taper off just before the national examinations, which are usually held during the fourth quarter of the year (the primary school national examinations are typically held around the first week of October, while the secondary and post-secondary national examinations are typically held around November). We suggest that, despite the overall increase in household expenditure on tuition over the past decade, the decreasing trend in monthly Google searches for "tuition" might be explained by the fact that student enrolment in primary and secondary schools has generally been declining over the past decade due to the falling birth rate of Singapore.      

## Building the NDLM and Performing Bayesian Analysis

As explained earlier, the plot of the observed data suggested a model consisting of a polynomial component and a seasonal component. To keep the model simple, we adopted a linear trend component. The period of the seasonal component was set as 12 months, reflecting the varying levels of demand for tuition throughout each academic year. The dataset contained 160 observations of normalized monthly Google searches for "tuition", starting from January 2010 and ending on April 2023. The model was fitted to the first 155 observations, while the last 5 observations were used to assess the accuracy of the forecast function. The observational variance was assumed to be constant but unknown, its conditional prior distribution following an inverse Gamma distribution with one degree of freedom and the prior estimate of the observational variance equal to 500. The system covariance was also assumed to be unknown, and was specified via a discount factor in the range $(0.7, 1]$ by using the mean squared errors of the forecast values as the criterion for selecting the discount value. Based on this criterion, the optimal discount value was found to be 0.93. The parameter vector $\bf{\theta_t}$, which has length 9 - one parameter for the linear component and 8 for the seasonal component, was assumed to have a normally distributed prior with mean $\begin{bmatrix} 58.13125 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0  \end{bmatrix}^T$ and covariance matrix equal to $10\cdot I_9$. The prior estimate  of 58.13125 for the mean of the linear component parameter was obtained from the mean of the dataset.         

```{r, chunk-one, include=FALSE}
model_seasonal=dlmModTrig(s=12,q=4,dV=0,dW=1)
model_trend=dlmModPoly(order=1,dV=10,dW=rep(1,1),m0=c(mean(as.double(res1$`tuition: (Singapore)`))))
model=model_trend+model_seasonal
model$C0=10*diag(9)
n0=1
S0=500
k=length(model$m0)
T=length(data$yt)

Ft=array(1,c(1,k,T+l))
Gt=array(1,c(k,k,T+l))
for(t in 1:(T+l)){
  Ft[,,t]=model$FF
  Gt[,,t]=model$GG
}

## create list for matrices
set_up_dlm_matrices_unknown_v <- function(Ft, Gt, Wt_star){
  if(!is.array(Gt)){
    Stop("Gt and Ft should be array")
  }
  if(missing(Wt_star)){
    return(list(Ft=Ft, Gt=Gt))
  }else{
    return(list(Ft=Ft, Gt=Gt, Wt_star=Wt_star))
  }
}


## create list for initial states
set_up_initial_states_unknown_v <- function(m0, C0_star, n0, S0){
  return(list(m0=m0, C0_star=C0_star, n0=n0, S0=S0))
}

forward_filter_unknown_v <- function(data, matrices, 
                              initial_states, delta){
  ## retrieve dataset
  yt <- data$yt
  T<- length(yt)
  
  ## retrieve matrices
  Ft <- matrices$Ft
  Gt <- matrices$Gt
  if(missing(delta)){
    Wt_star <- matrices$Wt_star
  }
  
  ## retrieve initial state
  m0 <- initial_states$m0
  C0_star <- initial_states$C0_star
  n0 <- initial_states$n0
  S0 <- initial_states$S0
  C0 <- S0*C0_star
  
  ## create placeholder for results
  d <- dim(Gt)[1]
  at <- matrix(0, nrow=T, ncol=d)
  Rt <- array(0, dim=c(d, d, T))
  ft <- numeric(T)
  Qt <- numeric(T)
  mt <- matrix(0, nrow=T, ncol=d)
  Ct <- array(0, dim=c(d, d, T))
  et <- numeric(T)
  nt <- numeric(T)
  St <- numeric(T)
  dt <- numeric(T)
  
  # moments of priors at t
  for(i in 1:T){
    if(i == 1){
      at[i, ] <- Gt[, , i] %*% m0
      Pt <- Gt[, , i] %*% C0 %*% t(Gt[, , i])
      Pt <- 0.5*Pt + 0.5*t(Pt)
      if(missing(delta)){
        Wt <- Wt_star[, , i]*S0
        Rt[, , i] <- Pt + Wt
        Rt[,,i] <- 0.5*Rt[,,i]+0.5*t(Rt[,,i])
      }else{
        Rt[, , i] <- Pt/delta
        Rt[,,i] <- 0.5*Rt[,,i]+0.5*t(Rt[,,i])
      }
      
    }else{
      at[i, ] <- Gt[, , i] %*% t(mt[i-1, , drop=FALSE])
      Pt <- Gt[, , i] %*% Ct[, , i-1] %*% t(Gt[, , i])
      if(missing(delta)){
        Wt <- Wt_star[, , i] * St[i-1]
        Rt[, , i] <- Pt + Wt
        Rt[,,i]=0.5*Rt[,,i]+0.5*t(Rt[,,i])
      }else{
        Rt[, , i] <- Pt/delta
        Rt[,,i] <- 0.5*Rt[,,i]+0.5*t(Rt[,,i])
      }
    }
    
    # moments of one-step forecast:
    ft[i] <- t(Ft[, , i]) %*% t(at[i, , drop=FALSE]) 
    Qt[i] <- t(Ft[, , i]) %*% Rt[, , i] %*% Ft[, , i] + 
      ifelse(i==1, S0, St[i-1])
    et[i] <- yt[i] - ft[i]
    
    nt[i] <- ifelse(i==1, n0, nt[i-1]) + 1
    St[i] <- ifelse(i==1, S0, 
                    St[i-1])*(1 + 1/nt[i]*(et[i]^2/Qt[i]-1))
    
    # moments of posterior at t:
    At <- Rt[, , i] %*% Ft[, , i] / Qt[i]
    
    mt[i, ] <- at[i, ] + t(At) * et[i]
    Ct[, , i] <- St[i]/ifelse(i==1, S0, 
                  St[i-1])*(Rt[, , i] - Qt[i] * At %*% t(At))
    Ct[,,i] <- 0.5*Ct[,,i]+0.5*t(Ct[,,i])
  }
  cat("Forward filtering is completed!\n")
  return(list(mt = mt, Ct = Ct,  at = at, Rt = Rt, 
              ft = ft, Qt = Qt,  et = et, 
              nt = nt, St = St))
}

### smoothing function ###
backward_smoothing_unknown_v <- function(data, matrices, 
                                posterior_states,delta){
  ## retrieve data 
  yt <- data$yt
  T <- length(yt) 
  
  ## retrieve matrices
  Ft <- matrices$Ft
  Gt <- matrices$Gt
  
  ## retrieve matrices
  mt <- posterior_states$mt
  Ct <- posterior_states$Ct
  Rt <- posterior_states$Rt
  nt <- posterior_states$nt
  St <- posterior_states$St
  at <- posterior_states$at
  
  ## create placeholder for posterior moments 
  mnt <- matrix(NA, nrow = dim(mt)[1], ncol = dim(mt)[2])
  Cnt <- array(NA, dim = dim(Ct))
  fnt <- numeric(T)
  Qnt <- numeric(T)
  
  for(i in T:1){
    if(i == T){
      mnt[i, ] <- mt[i, ]
      Cnt[, , i] <- Ct[, , i]
    }else{
      if(missing(delta)){
        inv_Rtp1 <- chol2inv(chol(Rt[, , i+1]))
        Bt <- Ct[, , i] %*% t(Gt[, , i+1]) %*% inv_Rtp1
        mnt[i, ] <- mt[i, ] + Bt %*% (mnt[i+1, ] - at[i+1, ])
        Cnt[, , i] <- Ct[, , i] + Bt %*% (Cnt[, , i+1] - 
                                    Rt[, , i+1]) %*% t(Bt)
        Cnt[,,i] <- 0.5*Cnt[,,i]+0.5*t(Cnt[,,i])
      }else{
        inv_Gt <- solve(Gt[, , i+1])
        mnt[i, ] <- (1-delta)*mt[i, ] + 
                delta*inv_Gt %*% t(mnt[i+1, ,drop=FALSE])
        Cnt[, , i] <- (1-delta)*Ct[, , i] + 
                delta^2*inv_Gt %*% Cnt[, , i + 1]  %*% t(inv_Gt)
        Cnt[,,i] <- 0.5*Cnt[,,i]+0.5*t(Cnt[,,i])
      }
    }
    fnt[i] <- t(Ft[, , i]) %*% t(mnt[i, , drop=FALSE])
    Qnt[i] <- t(Ft[, , i]) %*% t(Cnt[, , i]) %*% Ft[, , i]
  }
  for(i in 1:T){
     Cnt[,,i]=St[T]*Cnt[,,i]/St[i] 
     Qnt[i]=St[T]*Qnt[i]/St[i]
  }
  cat("Backward smoothing is completed!\n")
  return(list(mnt = mnt, Cnt = Cnt, fnt=fnt, Qnt=Qnt))
}

## Forecast Distribution for k step
forecast_function_unknown_v <- function(posterior_states, k, 
                                        matrices, delta){
  
  ## retrieve matrices
  Ft <- matrices$Ft
  Gt <- matrices$Gt
  if(missing(delta)){
    Wt_star <- matrices$Wt_star
  }
  
  mt <- posterior_states$mt
  Ct <- posterior_states$Ct
  St <- posterior_states$St
  at <- posterior_states$at
  
  ## set up matrices
  T <- dim(mt)[1] # time points
  d <- dim(mt)[2] # dimension of state parameter vector
  
  ## placeholder for results
  at <- matrix(NA, nrow = k, ncol = d)
  Rt <- array(NA, dim=c(d, d, k))
  ft <- numeric(k)
  Qt <- numeric(k)
  
  for(i in 1:k){
    ## moments of state distribution
    if(i == 1){
      at[i, ] <- Gt[, , T+i] %*% t(mt[T, , drop=FALSE])
      
      if(missing(delta)){
       Rt[, , i] <- Gt[, , T+i] %*% Ct[, , T] %*% 
         t(Gt[, , T+i]) + St[T]*Wt_star[, , T+i]
      }else{
        Rt[, , i] <- Gt[, , T+i] %*% Ct[, , T] %*% 
          t(Gt[, , T+i])/delta
      }
      Rt[,,i] <- 0.5*Rt[,,i]+0.5*t(Rt[,,i])
      
    }else{
      at[i, ] <- Gt[, , T+i] %*% t(at[i-1, , drop=FALSE])
      if(missing(delta)){
        Rt[, , i] <- Gt[, , T+i] %*% Rt[, , i-1] %*% 
          t(Gt[, , T+i]) + St[T]*Wt_star[, , T + i]
      }else{
        Rt[, , i] <- Gt[, , T+i] %*% Rt[, , i-1] %*% 
          t(Gt[, , T+i])/delta
      }
      Rt[,,i] <- 0.5*Rt[,,i]+0.5*t(Rt[,,i])
    }
    
    
    ## moments of forecast distribution
    ft[i] <- t(Ft[, , T+i]) %*% t(at[i, , drop=FALSE])
    Qt[i] <- t(Ft[, , T+i]) %*% Rt[, , i] %*% Ft[, , T+i] + 
      St[T]
  }
  cat("Forecasting is completed!\n") # indicator of completion
  return(list(at=at, Rt=Rt, ft=ft, Qt=Qt))
}

## obtain 95% credible interval
get_credible_interval_unknown_v <- function(ft, Qt, nt, 
                                   quantile = c(0.025, 0.975)){
  bound <- matrix(0, nrow=length(ft), ncol=2)

  if ((length(nt)==1)){
   for (t in 1:length(ft)){
      t_quantile <- qt(quantile[1], df = nt)
      bound[t, 1] <- ft[t] + t_quantile*sqrt(as.numeric(Qt[t])) 
  
  # upper bound of 95% credible interval
      t_quantile <- qt(quantile[2], df = nt)
      bound[t, 2] <- ft[t] + 
        t_quantile*sqrt(as.numeric(Qt[t]))}
  }else{
  # lower bound of 95% credible interval
    for (t in 1:length(ft)){
      t_quantile <- qt(quantile[1], df = nt[t])
      bound[t, 1] <- ft[t] + 
        t_quantile*sqrt(as.numeric(Qt[t])) 
  
  # upper bound of 95% credible interval
      t_quantile <- qt(quantile[2], df = nt[t])
      bound[t, 2] <- ft[t] + 
        t_quantile*sqrt(as.numeric(Qt[t]))}
  }
  return(bound)

}


##################################################
##### using discount factor ##########
##################################################
## compute measures of forecasting accuracy
## MAD: mean absolute deviation
## MSE: mean square error
## MAPE: mean absolute percentage error
## Neg LL: Negative log-likelihood of disc,
##         based on the one step ahead forecast distribution
measure_forecast_accuracy <- function(et, yt, Qt=NA, nt=NA, type){
  if(type == "MAD"){
    measure <- mean(abs(et))
  }else if(type == "MSE"){
    measure <- mean(et^2)
  }else if(type == "MAPE"){
    measure <- mean(abs(et)/yt)
  }else if(type == "NLL"){
    measure <- log_likelihood_one_step_ahead(et, Qt, nt)
  }else{
    stop("Wrong type!")
  }
  return(measure)
}


## compute log likelihood of one step ahead forecast function
log_likelihood_one_step_ahead <- function(et, Qt, nt){
  ## et:the one-step-ahead error
  ## Qt: variance of one-step-ahead forecast function
  ## nt: degrees freedom of t distribution
  T <- length(et)
  aux=0
  for (t in 1:T){
    zt=et[t]/sqrt(Qt[t])
    aux=(dt(zt,df=nt[t],log=TRUE)-log(sqrt(Qt[t]))) + aux 
  } 
  return(-aux)
}

## Maximize log density of one-step-ahead forecast function to select discount factor
adaptive_dlm <- function(data, predict_size, matrices, initial_states, df_range, type, 
                         forecast=TRUE){
  measure_best <- NA
  measure <- numeric(length(df_range))
#  valid_data <- data$valid_data
  df_opt <- NA
  j <- 0
  ## find the optimal discount factor
  for(i in df_range){
    j <- j + 1
    results_tmp <- forward_filter_unknown_v(data, matrices, initial_states, i)
     
    measure[j] <- measure_forecast_accuracy(et=results_tmp$et, yt=data$yt,
                                  Qt=results_tmp$Qt, 
                                  nt=c(initial_states$n0,results_tmp$nt), type=type)
    
    
    if(j == 1){
      measure_best <- measure[j]
      results_filtered <- results_tmp
      df_opt <- i
    }else if(measure[j] < measure_best){
      measure_best <- measure[j]
      results_filtered <- results_tmp
      df_opt <- i
    }
  }
  results_smoothed <- backward_smoothing_unknown_v(data, matrices, results_filtered, delta = df_opt)
  if(forecast){
    results_forecast <- forecast_function_unknown_v(results_filtered, predict_size, 
                                          matrices, df_opt)
    return(list(results_filtered=results_filtered, 
                results_smoothed=results_smoothed, 
                results_forecast=results_forecast, 
                df_opt = df_opt, measure=measure))
  }else{
    return(list(results_filtered=results_filtered, 
                results_smoothed=results_smoothed, 
                df_opt = df_opt, measure=measure))
  }
  
}

matrices=set_up_dlm_matrices_unknown_v(Ft=Ft,Gt=Gt)
initial_states=set_up_initial_states_unknown_v(model$m0,
                                               model$C0,n0,S0)

df_range=seq(0.7,1,by=0.005)

## fit discount DLM
## MSE
results_MSE <- adaptive_dlm(data, l, matrices, 
                            initial_states, df_range,"MSE",forecast=TRUE)

## retrieve filtered results
results_filtered <- results_MSE$results_filtered
ci_filtered <- get_credible_interval_unknown_v(
  results_filtered$ft,results_filtered$Qt,results_filtered$nt)

#dim(results_filtered$mt)

## retrieve smoothed results
results_smoothed <- results_MSE$results_smoothed
ci_smoothed <- get_credible_interval_unknown_v(
  results_smoothed$fnt, results_smoothed$Qnt, 
  results_filtered$nt[length(results_smoothed$fnt)])

## one-step ahead forecasting
results_forecast=results_MSE$results_forecast
ci_forecast=get_credible_interval_unknown_v(results_forecast$ft, 
                                            results_forecast$Qt, 
                                            results_filtered$nt[T])
```

After fully specifying the model and setting up the prior distributions of the state parameter and observational variance conditioned on the initial data, we performed a Bayesian analysis to derive the filtering, smoothing and forecasting distributions. The first graph shows a plot of the actual observations (as black circles) and a plot of the mean values of the smoothing distributions (in blue) together with the corresponding 95% credible intervals (in red).

```{r, chunk-two, message=FALSE, echo=FALSE, results='hide'}

## plot smoothing results 
par(mfrow=c(1,1))
index <- res1$YearMonth
index_smoothed=index[1:T]
index_forecast=index[(T+1):(T+l)]

plot(index, as.double(res1$`tuition: (Singapore)`), ylab='Google hits',
     main = "Google Trends: tuition (Smoothing Distributions)", type = 'l',
     xlab = 'time', lty=3,ylim=c(0,100))
points(index, as.double(res1$`tuition: (Singapore)`), pch=20)
lines(index_smoothed, results_smoothed$fnt, type = 'l', col='blue', 
      lwd=2)
lines(index_smoothed, ci_smoothed[, 1], type='l', col='red', lty=2)
lines(index_smoothed, ci_smoothed[, 2], type='l', col='red', lty=2)
```

The second graph shows a plot of the actual observations (as black circles) and a plot of the mean values of the filtering distributions (in pink) together with the corresponding 95% credible intervals (in blue).

```{r, message=FALSE, echo=FALSE, results='hide'}
## plot filtering results 
par(mfrow=c(1,1))
plot(index, as.double(res1$`tuition: (Singapore)`), ylab='Google hits',
     main = "Google Trends: tuition (Filtering Distributions)", type = 'l',
     xlab = 'time', lty=3,ylim=c(0,100))
points(index, as.double(res1$`tuition: (Singapore)`), pch=20)
lines(index_smoothed,results_filtered$ft, type='l', col='pink',lwd=2)
lines(index_smoothed,ci_filtered[, 1], type='l', col='blue', lty=2)
lines(index_smoothed,ci_filtered[, 2], type='l', col='blue', lty=2)
```

Finally, the third graph shows a plot of the actual observations (as black circles) and a plot of the means of the forecasting distributions (in green) together with the corresponding 95% credible intervals (in green).  

```{r, message=FALSE, echo=FALSE, results='hide'}
## plot forecasting results
par(mfrow=c(1,1))
index <- res1$YearMonth

index_forecast=index[156:160]

plot(index, as.double(res1$`tuition: (Singapore)`), ylab='Google hits',
     main = "Google Trends: tuition (Forecasting Distributions)", type = 'l',
     xlab = 'time', lty=3,ylim=c(0,100))
points(index, as.double(res1$`tuition: (Singapore)`), pch=20)
lines(index_forecast, results_forecast$ft, type='l', 
      col='green',lwd=2)
lines(index_forecast, ci_forecast[, 1], type='l', 
      col='green', lty=2)
lines(index_forecast, ci_forecast[, 2], type='l', 
      col='green', lty=2)
```

## Conclusion

Perhaps not surprisingly, the plot of the means of the smoothing distributions was smoother than the corresponding plot for the filtering distributions. The size of the 95% credible intervals for the forecasting distributions also appeared to increase as the number of steps ahead increased, indicating an increase in uncertainty. The mean squared errors for the smoothing, filtering and forecasting distributions were, respectively, 17.03985, 63.00045 and 42.76996. We conclude that the model consisting of a linearly decreasing trend and a seasonal component with period 12 months and  4 harmonics appears to fit the data to some extent. That is to say, Google searches for the term "tuition" in Singapore do appear to have been declining since 2010 and follow a seasonal trend with a fundamental period of 12 months. However, one limitation of this model is that the coefficient matrices $F_t$ and $G_t$ are assumed to be constant over time. We also assumed that the observation variance and the system covariance are independent of each other, which may not hold in practice.   
